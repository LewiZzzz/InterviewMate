# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ AI é¢è¯•åŠ©æ‰‹")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download

model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16  # A10 GPUæ”¯æŒ


# torch_dtype = torch.float16 # P100 GPUæ”¯æŒ

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("æ­£åœ¨åˆ›å»º tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(
        ['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
         '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>',
         '<jupyter_code>', '<jupyter_output>', '<empty_output>'],
        special_tokens=True
    )

    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()

    print("æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")
    return tokenizer, model


# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

# ä½¿ç”¨ radio ç»„ä»¶å®ç°æ»‘å—å¼é¡µé¢åˆ‡æ¢
page = st.radio("é€‰æ‹©é¡µé¢", ["æ¨¡æ‹Ÿé¢è¯•", "å‚è€ƒé¢è¯•"])


# å®šä¹‰Promptæ¨¡æ¿
def create_prompt_template(prompt_type, user_input, role=None):
    if prompt_type == "interview_simulation":
        if role:
            return f"ä½ ç°åœ¨æ˜¯ä¸€ä½é¢è¯•å®˜ï¼Œé¢è¯•æ±‚èŒè€…åº”è˜çš„èŒä½æ˜¯{role}ã€‚è¯·åªç”Ÿæˆä¸€ä¸ªä¸æ­¤èŒä½ç›¸å…³çš„é¢è¯•é—®é¢˜ã€‚"
        else:
            return "ä½ ç°åœ¨æ˜¯ä¸€ä½é¢è¯•å®˜ã€‚è¯·é¦–å…ˆè¯¢é—®æ±‚èŒè€…åº”è˜çš„å²—ä½ã€‚"
    elif prompt_type == "evaluate_answer":
        return f"ä½ ç°åœ¨æ˜¯ä¸€ä½é¢è¯•å®˜ã€‚æ±‚èŒè€…çš„åº”è˜å²—ä½æ˜¯{role}ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ±‚èŒè€…çš„å›ç­”ï¼Œè¯„ä¼°å…¶å›ç­”è´¨é‡å¹¶æä¾›åé¦ˆã€‚\n\næ±‚èŒè€…çš„å›ç­”: {user_input}\n\nè¯„ä»·ä¸åé¦ˆ:"
    elif prompt_type == "reference_interview":
        return f"é¢è¯•é—®é¢˜: {user_input}\n\nå‚è€ƒç­”æ¡ˆ:"


if page == "æ¨¡æ‹Ÿé¢è¯•":
    st.header("æ¨¡æ‹Ÿé¢è¯•")

    # åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
        st.session_state["interview_role"] = None  # å­˜å‚¨æ±‚èŒè€…çš„ç›®æ ‡èŒä½
        st.session_state["current_question"] = None  # å­˜å‚¨å½“å‰é¢è¯•é—®é¢˜

    # å¦‚æœæ±‚èŒè€…è¿˜æ²¡æœ‰è¾“å…¥åº”è˜å²—ä½ï¼Œé¦–å…ˆè¯¢é—®å²—ä½
    if st.session_state["interview_role"] is None:
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨åº”è˜çš„å²—ä½:"):
            st.session_state["interview_role"] = prompt
            st.session_state.messages.append({"role": "user", "content": f"åº”è˜å²—ä½: {prompt}"})
            st.chat_message("user").write(f"åº”è˜å²—ä½: {prompt}")

            # ç”Ÿæˆé¢è¯•å®˜çš„é¦–ä¸ªé—®é¢˜
            full_prompt = create_prompt_template("interview_simulation", "", role=st.session_state["interview_role"])
            inputs = tokenizer(full_prompt, return_tensors="pt")["input_ids"].cuda()

            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆé—®é¢˜
            outputs = model.generate(inputs, do_sample=False, max_length=1024)  # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
            output = tokenizer.decode(outputs[0])
            question = output.split("<sep>")[-1].replace("<eod>", '').strip()

            # ä¿å­˜å¹¶æ˜¾ç¤ºé—®é¢˜
            st.session_state["current_question"] = question
            st.session_state.messages.append({"role": "assistant", "content": question})
            st.chat_message("assistant").write(question)
    else:
        # æ˜¾ç¤ºå½“å‰é—®é¢˜
        if st.session_state["current_question"]:
            st.chat_message("assistant").write(st.session_state["current_question"])

        # å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å›ç­”ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
        if answer := st.chat_input("è¯·è¾“å…¥æ‚¨çš„å›ç­”:"):
            # å°†ç”¨æˆ·çš„å›ç­”æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
            st.session_state.messages.append({"role": "user", "content": answer})
            st.chat_message("user").write(answer)

            # ä½¿ç”¨ Prompt æ¨¡æ¿ç”Ÿæˆè¯„ä»·è¾“å…¥
            evaluation_prompt = create_prompt_template("evaluate_answer", answer,
                                                       role=st.session_state["interview_role"])
            inputs = tokenizer(evaluation_prompt, return_tensors="pt")["input_ids"].cuda()

            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆè¯„ä»·
            outputs = model.generate(inputs, do_sample=False, max_length=512)  # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
            output = tokenizer.decode(outputs[0])
            evaluation = output.split("<sep>")[-1].replace("<eod>", '').strip()

            # æ˜¾ç¤ºè¯„ä»·
            st.session_state.messages.append({"role": "assistant", "content": evaluation})
            st.chat_message("assistant").write(evaluation)

            # é‡ç½®å½“å‰é—®é¢˜ä»¥ä¾¿ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
            st.session_state["current_question"] = None

elif page == "å‚è€ƒé¢è¯•":
    st.header("å‚è€ƒé¢è¯•")

    # åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"reference_messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
    if "reference_messages" not in st.session_state:
        st.session_state["reference_messages"] = []

    # éå†å¹¶æ˜¾ç¤ºæ‰€æœ‰èŠå¤©è®°å½•
    for msg in st.session_state.reference_messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†é—®é¢˜ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨æƒ³å’¨è¯¢çš„é¢è¯•é—®é¢˜:"):
        # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„reference_messagesåˆ—è¡¨ä¸­
        st.session_state.reference_messages.append({"role": "user", "content": question})

        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
        st.chat_message("user").write(question)

        # ä½¿ç”¨ Prompt æ¨¡æ¿ç”Ÿæˆæ¨¡å‹è¾“å…¥
        full_prompt = create_prompt_template("reference_interview", question)
        inputs = tokenizer(full_prompt, return_tensors="pt")["input_ids"].cuda()

        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå‚è€ƒç­”æ¡ˆ
        outputs = model.generate(inputs, do_sample=False, max_length=512)  # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
        output = tokenizer.decode(outputs[0])
        reference_answer = output.split("<sep>")[-1].replace("<eod>", '').strip()

        # åˆ é™¤å‚è€ƒç­”æ¡ˆä¹‹å‰çš„å†…å®¹ï¼Œåªä¿ç•™æ ¸å¿ƒç­”æ¡ˆéƒ¨åˆ†
        reference_answer = reference_answer.split("å‚è€ƒç­”æ¡ˆ:")[-1].strip()

        # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„reference_messagesåˆ—è¡¨ä¸­
        st.session_state.reference_messages.append({"role": "assistant", "content": reference_answer})

        # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
        st.chat_message("assistant").write(reference_answer)
